

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Surrogates &mdash; UQit 1.0.1 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="UQ Forward Problem" href="uqFWD.html" />
    <link rel="prev" title="Sampling" href="sampling.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> UQit
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="instl_dep.html">Installation and Dependencies</a></li>
<li class="toctree-l1"><a class="reference internal" href="codes_list.html">List of core codes</a></li>
<li class="toctree-l1"><a class="reference internal" href="terminology.html">Overview &amp; Terminology</a></li>
<li class="toctree-l1"><a class="reference internal" href="sampling.html">Sampling</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Surrogates</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#non-intrusive-polynomial-chaos-expansion">Non-intrusive Polynomial Chaos Expansion</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lagrange-interpolation">Lagrange Interpolation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#theory">Theory</a></li>
<li class="toctree-l3"><a class="reference internal" href="#example">Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-lagInt">Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#notebook">Notebook</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#gaussian-process-regression">Gaussian Process Regression</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id9">Theory</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id14">Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id15">Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id17">Notebook</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="uqFWD.html">UQ Forward Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="gsa.html">Global Sensitivity Analysis (GSA)</a></li>
<li class="toctree-l1"><a class="reference internal" href="others.html">Other Core Codes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bib.html">Bibliography</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">UQit</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Surrogates</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/UQit_core_/surrogate.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="surrogates">
<span id="surrogates-sect"></span><h1>Surrogates<a class="headerlink" href="#surrogates" title="Permalink to this headline">¶</a></h1>
<p>A surrogate or metamodel is an approximation of the actual model function or simulator over the parameter space.
Running a surrogate is computationally much less expensive than the actual simulator, a characteristic that makes the use of surrogates inevitable in different UQ problems.
However, the predictions by the surrogate should be accurate enough compared to the actual predictions by the simulator.
Using an additive error model, the following relation can be considered between the model function (simulator) and its observations:</p>
<div class="math notranslate nohighlight">
\[r=f(\chi,\mathbf{q})+\mathbf{\varepsilon}\,,\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{\varepsilon}\)</span> expresses the bias and random noise.
Our task is to construct a surrogate <span class="math notranslate nohighlight">\(\tilde{f}(\chi,\mathbf{q})\)</span> for the actual model function (simulator).</p>
<p>Treating the simulator as a blackbox, a set of training data <span class="math notranslate nohighlight">\(\mathcal{D}=\{(\mathbf{q}^{(i)},r^{(i)})\}_{i=1}^n\)</span> is obtained.
There are different techniques to construct a surrogate, for instance see <a class="reference internal" href="../bib.html#smith13" id="id1"><span>[Smith13]</span></a>, <a class="reference internal" href="../bib.html#ghanem17" id="id2"><span>[Ghanem17]</span></a>, and <a class="reference internal" href="../bib.html#gramacy20" id="id3"><span>[Gramacy20]</span></a>.
Some of the techniques relevant to CFD applications have been implemented in <code class="code docutils literal notranslate"><span class="pre">UQit</span></code>.
Here, we provide a short overview to the theory behind these techniques, explain their implementation in <code class="code docutils literal notranslate"><span class="pre">UQit</span></code>, and provide examples to show how to use them.</p>
<div class="section" id="non-intrusive-polynomial-chaos-expansion">
<h2>Non-intrusive Polynomial Chaos Expansion<a class="headerlink" href="#non-intrusive-polynomial-chaos-expansion" title="Permalink to this headline">¶</a></h2>
<p>As a type of stochastic collocation (SC) methods, see e.g. Chapter 20 in <a class="reference internal" href="../bib.html#ghanem17" id="id4"><span>[Ghanem17]</span></a>, non-intrusive PCE <a class="reference internal" href="../bib.html#xiu05" id="id5"><span>[Xiu05]</span></a>, <a class="reference internal" href="../bib.html#xiu07" id="id6"><span>[Xiu07]</span></a> can be used to construct a surrogate.</p>
<div class="math notranslate nohighlight">
\[\tilde{f}(\chi,\mathbf{q}) = \sum_{k=0}^K \hat{f}_k(\chi) \Psi_{k}(\xi) \,.\]</div>
<p>There is a one-to-one correspondence between any sample of <span class="math notranslate nohighlight">\(\mathbf{q}\in \mathbb{Q}\)</span> and <span class="math notranslate nohighlight">\(\xi\in\Gamma\)</span>, where <span class="math notranslate nohighlight">\(\mathbb{Q}=\bigotimes_{i=1}^p \mathbb{Q}_i\)</span> and <span class="math notranslate nohighlight">\(\Gamma=\bigotimes_{i=1}^p \Gamma_i\)</span>.
Note that <span class="math notranslate nohighlight">\(\mathbb{Q}_i\)</span> is the admissible space of the i-th parameter which can be mappd onto <span class="math notranslate nohighlight">\(\Gamma_i\)</span> based on the gPCE rules, see <a class="reference internal" href="../bib.html#xiu02" id="id7"><span>[Xiu02]</span></a>, <a class="reference internal" href="../bib.html#eldred09" id="id8"><span>[Eldred09]</span></a>.
For the details of the non-intrusive PCE method refer to <a class="reference internal" href="uqFWD.html#gpce-sect"><span class="std std-ref">Standard Polynomial Chaos Expansion</span></a>.</p>
</div>
<div class="section" id="lagrange-interpolation">
<h2>Lagrange Interpolation<a class="headerlink" href="#lagrange-interpolation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="theory">
<h3>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h3>
<p>As another form of SC-based surrogates, Lagrange interpolation can be considered:</p>
<div class="math notranslate nohighlight">
\[\tilde{f}(\chi,\mathbf{q}) = \sum_{k=1}^n \hat{f}_k(\chi,\mathbf{q}) L_k(\mathbf{q}) \,,\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{f}_k(\chi,\mathbf{q})=f(\chi,\mathbf{q}^{(k)})=r^{(k)}\)</span> are the training model outputs.
If the <span class="math notranslate nohighlight">\(n_i\)</span> samples taken from the <span class="math notranslate nohighlight">\(i\)</span>-th parameter space are represented by <span class="math notranslate nohighlight">\(Q_{n_i}\)</span>, then the use of tensor-product leads to the nodal set <span class="math notranslate nohighlight">\(Q_n\)</span> of size <span class="math notranslate nohighlight">\(n=\prod_{i=1}^p n_i\)</span>, where,</p>
<div class="math notranslate nohighlight">
\[Q_n= Q_{n_1} \bigotimes Q_{n_2}\bigotimes \ldots \bigotimes Q_{n_p} \,.\]</div>
<p>Correspondingly, the Lagrange bases <span class="math notranslate nohighlight">\(L_k(\mathbf{q})\)</span> are constructed using the tensor-product of the bases in each of the parameter spaces:</p>
<div class="math notranslate nohighlight">
\[L_k(\mathbf{q})=L_{k_1}(q_1) \bigotimes L_{k_2}(q_2) \bigotimes \ldots \bigotimes L_{k_p}(q_p) \,,\]</div>
<p>where,</p>
<div class="math notranslate nohighlight">
\[\begin{split}L_{k_i}(q_i) = \prod_{\substack{{k_i=1}\\{k_i\neq j}}}^{n_i}
\frac{q_i - q_i^{(k_i)}}{q_i^{(k_i)}-q_i^{(j)}} \,,\quad i=1,2,\ldots,p \,.\end{split}\]</div>
<p>Note that the Lagrange basis satisfies <span class="math notranslate nohighlight">\(L_{k_i}(q_i^{(j)})=\delta_{k_{i}j}\)</span>, where <span class="math notranslate nohighlight">\(\delta\)</span> represents the Kronecker delta.</p>
</div>
<div class="section" id="example">
<h3>Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>For <span class="math notranslate nohighlight">\(p=1\)</span> (one-dimensional parameter <span class="math notranslate nohighlight">\(q\)</span>):</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fInterp</span><span class="o">=</span><span class="n">lagInt</span><span class="p">(</span><span class="n">fNodes</span><span class="o">=</span><span class="n">fNodes</span><span class="p">,</span><span class="n">qNodes</span><span class="o">=</span><span class="p">[</span><span class="n">qNodes</span><span class="p">],</span><span class="n">qTest</span><span class="o">=</span><span class="p">[</span><span class="n">qTest</span><span class="p">])</span><span class="o">.</span><span class="n">val</span>
</pre></div>
</div>
<ul class="simple">
<li><p>For <span class="math notranslate nohighlight">\(p&gt;1\)</span> (multi-dimensional parameter <span class="math notranslate nohighlight">\(\mathbf{q}\)</span>):</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fInterp</span><span class="o">=</span><span class="n">lagInt</span><span class="p">(</span><span class="n">fNodes</span><span class="o">=</span><span class="n">fNodes</span><span class="p">,</span><span class="n">qNodes</span><span class="o">=</span><span class="n">qNodes</span><span class="p">,</span><span class="n">qTest</span><span class="o">=</span><span class="n">qTestList</span><span class="p">,</span><span class="n">liDict</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;testRule&#39;</span><span class="p">:</span><span class="s1">&#39;tensorProd&#39;</span><span class="p">})</span><span class="o">.</span><span class="n">val</span>
</pre></div>
</div>
</div>
<div class="section" id="module-lagInt">
<span id="implementation"></span><h3>Implementation<a class="headerlink" href="#module-lagInt" title="Permalink to this headline">¶</a></h3>
<dl class="simple">
<dt>Note: </dt><dd><ul class="simple">
<li><p>To avoid Runge’s phenomenon, the training nodal set should be non-uniformly distributed in each dimension of the parameter space.</p></li>
</ul>
</dd>
</dl>
<dl class="py class">
<dt id="lagInt.lagInt">
<em class="property">class </em><code class="sig-prename descclassname">lagInt.</code><code class="sig-name descname">lagInt</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">qNodes</span></em>, <em class="sig-param"><span class="n">fNodes</span></em>, <em class="sig-param"><span class="n">qTest</span></em>, <em class="sig-param"><span class="n">liDict</span><span class="o">=</span><span class="default_value">[]</span></em><span class="sig-paren">)</span><a class="headerlink" href="#lagInt.lagInt" title="Permalink to this definition">¶</a></dt>
<dd><p>Lagrange interpolation over a p-D parameter space, where p=1,2,…
The interpolation order is <span class="math notranslate nohighlight">\((n_1-1)*(n_2-1)*...*(n_p-1)\)</span>, where, 
<span class="math notranslate nohighlight">\(n_k, k=1,2,...,p\)</span> refers to the number of training nodes in the i-th dimension of the parameter space.</p>
<div class="math notranslate nohighlight">
\[F(\mathbf{Q})=\sum_{k_1=1}^{n_1} ... \sum_{k_p=1}^{n_p} [fNodes(k_1,k_2,...,k_p) L_{k_1}(Q_1) L_{k_2}(Q_2) ... L_{k_p}(Q_p)]\]</div>
<p>where, <span class="math notranslate nohighlight">\(L_{k_i}(Q_i)\)</span> is the single-variate Lagrange basis in the i-th dimension.</p>
<dl class="simple">
<dt>Args:      </dt><dd><dl class="simple">
<dt><cite>qNodes</cite>: A list of length p</dt><dd><p><cite>qNodes=[qNodes_1,qNodes_2,…,qNodes_p]</cite>, 
where <cite>qNodes_k</cite> is a 1D numpy array of <cite>n_k</cite> nodes in the k-th parameter dimension.</p>
</dd>
<dt><cite>fNodes</cite>: A p-D numpy array of shape <cite>(n_1,n_2,…,n_p)</cite> (or a 1D array of size <cite>n=n_1*n_2*…*n_p</cite> if p&gt;1)</dt><dd><p>Response values at the training nodes.</p>
</dd>
<dt><cite>qTest</cite>: A list of length p </dt><dd><p>Containing test samples from the p-D parameter space, i.e. <cite>qTest=[Q_0,Q_1,…,Q_{p-1}]</cite> 
where <cite>Q_i</cite> is a 1D numpy array of size <cite>m_i, i=0,1,…,p-1</cite>.
Note that <cite>Q_i</cite> must be in <cite>[min(qNodes_i),max(qNodes_i)]</cite> for <cite>i=1,2,…,p</cite>.</p>
</dd>
<dt><cite>liDict</cite>: dict (optional)</dt><dd><dl class="simple">
<dt>To set different options for Lagrange interpolation over the p-D space when p&gt;1, with the following key:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>‘testRule’: The rule for treating the multi-dimensional test points with the values:</dt><dd><ul>
<li><p>‘tensorProd’: A tensor-product grid is constructed from <cite>Q_i`s in list `qTest</cite>.
Hence, the total number of sample points is <cite>m=m0*m1*m_{p-1}</cite>.</p></li>
<li><p>‘set’: All <cite>m_i</cite> for <cite>i=0,1,…p-1</cite> should be equal. As a result, 
a set of <cite>m=m_i</cite> test points is considered.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</dd>
</dl>
</dd>
<dt>Returns: </dt><dd><dl class="simple">
<dt><cite>val</cite>: f(qTest), Interpolated values of f(q) at <cite>qTest</cite></dt><dd><ul class="simple">
<li><p>If p==1: 1D numpy array of size m1</p></li>
<li><p>If p&gt;1 and ‘testRule’==’tensorProd’ =&gt; <cite>val</cite>: pD numpy array of shape <cite>(m1,m2,…,mp)</cite></p></li>
<li><p>If p&gt;1 and ‘testRule’==’set’ =&gt; ‘val’: 1D numpy array of size <cite>m1*m2*…*mp</cite></p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt id="lagInt.lagInt.basis1d">
<code class="sig-name descname">basis1d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">qNodes_</span></em>, <em class="sig-param"><span class="n">k</span></em>, <em class="sig-param"><span class="n">Q_</span></em><span class="sig-paren">)</span><a class="headerlink" href="#lagInt.lagInt.basis1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructs single-variate Lagrange bases <span class="math notranslate nohighlight">\(L_k(q)\)</span> using <cite>n</cite> nodes <cite>qNodes_</cite> for 
<cite>k=0,1,…,n-1</cite>. The bases are evaluated at <cite>m</cite> samples <cite>Q_</cite> of a single-variate parameter</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt><cite>qNodes_</cite>: 1D numpy array of size n</dt><dd><p>The single-variate training nodal set</p>
</dd>
<dt><cite>Q_</cite>: 1D numpy array of size m</dt><dd><p>Test samples</p>
</dd>
<dt><cite>k</cite>: 1D numpy array of int values</dt><dd><p>Order of the polynomial bases</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="simple">
<dt><cite>prod</cite>: n-by-m numpy array</dt><dd><p>Values of <span class="math notranslate nohighlight">\(L_k(Q)\)</span> for <cite>k=0,1,…,n-1</cite> evaluated at <cite>Q_</cite></p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lagInt.lagInt.interp">
<code class="sig-name descname">interp</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#lagInt.lagInt.interp" title="Permalink to this definition">¶</a></dt>
<dd><p>Lagrange interpolation, for p=1,2, …</p>
</dd></dl>

<dl class="py method">
<dt id="lagInt.lagInt.interp_1d">
<code class="sig-name descname">interp_1d</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#lagInt.lagInt.interp_1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Lagrange interpolation of order (n-1) constructed over a 1D parameter space.
Bases are constructed from n nodes <cite>qNodes</cite> and are evaluated at test points <cite>Q</cite> in <cite>[min(qNodes_),max(qNodes_)]</cite>.</p>
<div class="math notranslate nohighlight">
\[F(Q)=\sum_{k=0}^{n-1} fNodes_{k} L_k(Q)\]</div>
<p>where, Lagrange Bases L_k(q) are constructed from the nodal set.</p>
</dd></dl>

<dl class="py method">
<dt id="lagInt.lagInt.interp_pd">
<code class="sig-name descname">interp_pd</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#lagInt.lagInt.interp_pd" title="Permalink to this definition">¶</a></dt>
<dd><p>Lagrange interpolation of order <span class="math notranslate nohighlight">\((n_1-1)*(n_2-1)*...*(n_p-1)\)</span> constructed over a 
p-D parameter space. Here, <span class="math notranslate nohighlight">\(n_k, k=1,2,...,p\)</span> refers to the number of training nodes in the i-th dimension of the parameter space.</p>
<div class="math notranslate nohighlight">
\[F(\mathbf{Q})=\sum_{k_1=1}^{n_1} ... \sum_{k_p=1}^{n_p} [fNodes(k_1,k_2,...,k_p) L_{k_1}(Q_1) L_{k_2}(Q_2) ... L_{k_p}(Q_p)]\]</div>
<p>where, <span class="math notranslate nohighlight">\(L_{k_i}(Q_p)\)</span> is the single-variate Lagrange basis in the i-th dimension.</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="lagInt.lagInt_Quads2Line">
<code class="sig-prename descclassname">lagInt.</code><code class="sig-name descname">lagInt_Quads2Line</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fNodes</span></em>, <em class="sig-param"><span class="n">qNodes</span></em>, <em class="sig-param"><span class="n">lineDef</span></em><span class="sig-paren">)</span><a class="headerlink" href="#lagInt.lagInt_Quads2Line" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructing a Lagrange interpolation from tensor-product nodal set in a 2D parameter space and then evaluating it at the test points located on a straight line lying in the same parameter plane.</p>
<dl class="simple">
<dt>Args: </dt><dd><dl class="simple">
<dt><cite>qNodes</cite>: A list of length 2</dt><dd><p><cite>qNodes=[Q1,Q2]</cite> list of training nodes <cite>Qi</cite> is a 1D numpy array of length ni for i=1,2.</p>
</dd>
<dt><cite>fNodes</cite>: 1D numpy array of length <cite>n1*n2</cite></dt><dd><p>Values of the response at <cite>qNodes</cite></p>
</dd>
<dt><cite>lineDef</cite>: dict</dt><dd><dl class="simple">
<dt>Defines the line over which the test samples are to be taken. The keys are:</dt><dd><ul class="simple">
<li><p><cite>‘lineStart’:[q1Start,q2Start]</cite>; line’s starting point</p></li>
<li><p><cite>‘lineEnd’:[q1End,q2End]</cite>; line’s end point</p></li>
<li><p><cite>‘noPtsLine’</cite>: int; number of test points on the line, <cite>m1</cite></p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</dd>
<dt>Returns:       </dt><dd><dl class="simple">
<dt><cite>val</cite>: f(qTest), Interpolated values of f(q) at <cite>qTest</cite></dt><dd><p>1D numpy array of size <cite>m1</cite></p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="notebook">
<h3>Notebook<a class="headerlink" href="#notebook" title="Permalink to this headline">¶</a></h3>
<p>Try this <a class="reference internal" href="../examples/lagInt.html"><span class="doc">LagInt notebook</span></a> to see how to use <code class="code docutils literal notranslate"><span class="pre">UQit</span></code> for Lagrange interpolation over a parameter space.</p>
</div>
</div>
<div class="section" id="gaussian-process-regression">
<h2>Gaussian Process Regression<a class="headerlink" href="#gaussian-process-regression" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id9">
<h3>Theory<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<p>Consider the simulator <span class="math notranslate nohighlight">\(f(\mathbf{q})\)</span> where <span class="math notranslate nohighlight">\(\mathbf{q}\in \mathbb{Q}\)</span>.
The observations are assumed to be generated from the following model,</p>
<div class="math notranslate nohighlight">
\[y = f(\mathbf{q}) + \varepsilon  \,.\]</div>
<p>Since the exact simulator <span class="math notranslate nohighlight">\(f(\mathbf{q})\)</span> is not known, we can put a prior on it, which is in the form of a Gaussian process, see <a class="reference internal" href="../bib.html#rasmussen05" id="id10"><span>[Rasmussen05]</span></a>, <a class="reference internal" href="../bib.html#gramacy20" id="id11"><span>[Gramacy20]</span></a>.
Based on the training data <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>, the posterior of the <span class="math notranslate nohighlight">\(f(q)\)</span>, denoted by <span class="math notranslate nohighlight">\(\tilde{f}(\mathbf{q})\)</span>, is inferred.
Without loss of generality we assume <span class="math notranslate nohighlight">\(\varepsilon\sim\mathcal{N}(0,\sigma^2)\)</span>.
Contrary to the common use of Gaussian process regression (GPR) where <span class="math notranslate nohighlight">\(\sigma\)</span> is assumed to be fixed for all observations (homoscedastic noise), we are interested in cases where <span class="math notranslate nohighlight">\(\sigma\)</span> is observation-dependent (heteroscedastic noise).
In the latter, we need to have a Gaussian process to infer the noise parameters, see <a class="reference internal" href="../bib.html#goldberg98" id="id12"><span>[Goldberg98]</span></a>.
Eventually, the posteriors of <span class="math notranslate nohighlight">\(\tilde{f}(\mathbf{q})\)</span> and response <span class="math notranslate nohighlight">\(y\)</span> can be sampled over the parameter space, see <a class="reference internal" href="../bib.html#rezaeiravesh20" id="id13"><span>[Rezaeiravesh20]</span></a> and the references therein for the details.</p>
</div>
<div class="section" id="id14">
<h3>Example<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h3>
<p>Given the training data including the observational noise, A GPR is constructed in <code class="code docutils literal notranslate"><span class="pre">UQit</span></code> as,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">gpr_</span><span class="o">=</span><span class="n">gpr</span><span class="p">(</span><span class="n">xTrain</span><span class="p">,</span><span class="n">yTrain</span><span class="p">[:,</span><span class="kc">None</span><span class="p">],</span><span class="n">noiseSdev</span><span class="p">,</span><span class="n">xTest</span><span class="p">,</span><span class="n">gprOpts</span><span class="p">)</span>
<span class="n">post_f</span><span class="o">=</span><span class="n">gpr_</span><span class="o">.</span><span class="n">post_f</span>
<span class="n">post_obs</span><span class="o">=</span><span class="n">gpr_</span><span class="o">.</span><span class="n">post_y</span>
</pre></div>
</div>
</div>
<div class="section" id="id15">
<h3>Implementation<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h3>
<p>In <code class="code docutils literal notranslate"><span class="pre">UQit</span></code>, the GPR is implemented using the existing Python library <code class="code docutils literal notranslate"><span class="pre">GPyTorch</span></code> <a class="reference internal" href="../bib.html#gardner18" id="id16"><span>[Gardner18]</span></a>.
The user can similarly use any other available library for GPR as long as the code structure is kept consistent with <code class="code docutils literal notranslate"><span class="pre">UQit</span></code>.</p>
<span class="target" id="module-gpr_torch"></span><dl>
<dt>Notes:</dt><dd><ol class="arabic simple">
<li><p>GPyTorch: Size of the training data cannot exceed 128.
Otherwise, make batches of max size 128.</p></li>
</ol>
<p>2. GPyTorch: If the standard deviation of the observation noise is 
exactly zero for all observations, then there may be issues with
Cholesky decomposition. Therefore, instead of zero, use a very small 
value for the noise standard deviations.</p>
<p>3. In a p-D parameter space, it is required to define a length-scale
per dimension. Based on the experience, if the range of the parameters
are too different from each other or are too large, the optimization
of the length-scales can be problematic. To rectify this, the original
parameter space can be mapped, for instance, into the hypercube
[-1,1]^p. Then, the GPR can be constructed on this mapped space.</p>
<p>4. Shifting and scaling (standardization) of the training data can result 
in a better fit of GPR. This can be activated via making <cite>‘standardizeYTrain’:True</cite> 
in <cite>gprOpts</cite>.</p>
<p>5. Different options for kernel function are available by <code class="code docutils literal notranslate"><span class="pre">GPyTorch</span></code>. 
One can change the default kernel in <cite>self.covar_module(…)</cite> (see below) 
manually by modifying the source code of <code class="code docutils literal notranslate"><span class="pre">UQit</span></code>.</p>
</dd>
</dl>
<dl class="py class">
<dt id="gpr_torch.SingletaskGPModel">
<em class="property">class </em><code class="sig-prename descclassname">gpr_torch.</code><code class="sig-name descname">SingletaskGPModel</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">train_x</span></em>, <em class="sig-param"><span class="n">train_y</span></em>, <em class="sig-param"><span class="n">likelihood</span></em><span class="sig-paren">)</span><a class="headerlink" href="#gpr_torch.SingletaskGPModel" title="Permalink to this definition">¶</a></dt>
<dd><p>GPR for single-task output using GPyTorch, 
1D input: y=f(x) in R, x in R</p>
<dl class="py method">
<dt id="gpr_torch.SingletaskGPModel.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#gpr_torch.SingletaskGPModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="gpr_torch.SingletaskGPModel_mIn">
<em class="property">class </em><code class="sig-prename descclassname">gpr_torch.</code><code class="sig-name descname">SingletaskGPModel_mIn</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">train_x</span></em>, <em class="sig-param"><span class="n">train_y</span></em>, <em class="sig-param"><span class="n">likelihood</span></em><span class="sig-paren">)</span><a class="headerlink" href="#gpr_torch.SingletaskGPModel_mIn" title="Permalink to this definition">¶</a></dt>
<dd><p>GPR for single-task output using GPyTorch, 
p-D input: y=f(x)  in R, x in R^p, p&gt;1</p>
<dl class="py method">
<dt id="gpr_torch.SingletaskGPModel_mIn.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#gpr_torch.SingletaskGPModel_mIn.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="gpr_torch.gpr">
<em class="property">class </em><code class="sig-prename descclassname">gpr_torch.</code><code class="sig-name descname">gpr</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">xTrain</span></em>, <em class="sig-param"><span class="n">yTrain</span></em>, <em class="sig-param"><span class="n">noiseV</span></em>, <em class="sig-param"><span class="n">xTest</span></em>, <em class="sig-param"><span class="n">gprOpts</span></em><span class="sig-paren">)</span><a class="headerlink" href="#gpr_torch.gpr" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructs and evaluates a GPR over the space of uncertain parameters/inputs. The GPR model is</p>
<div class="math notranslate nohighlight">
\[y=f(x)+\epsilon\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon\sim N(M,V)\)</span></p>
<blockquote>
<div><ul class="simple">
<li><p>The parameter (input) space is p-dimensional, where p=1,2,…</p></li>
<li><p>The response y is 1-D (single-task model).</p></li>
<li><p>The observations are assumed to be un-correlated, but the noise uncertainty 
can be observation-dependent (heteroscedastic). Therefore, only the diagonal
elements of V are (currently) assumed to be non-zero. If the noise uncertainty 
is fixed for all observations, then epsilon_i~iid (homoscedastic).</p></li>
</ul>
</div></blockquote>
<dl>
<dt>Args:          </dt><dd><dl>
<dt><cite>xTrain</cite>: 2D numpy array of size nxp</dt><dd><p>Training samples (size n) from the p-D input (parameter) space</p>
</dd>
<dt><cite>yTrain</cite>: 2D numpy array of size nxm (m: dimensionality of y)</dt><dd><p>Training model output (response)</p>
</dd>
<dt><cite>noiseV</cite>: A 1D numpy vecor of size n </dt><dd><p>Standard deviation of the the Gaussian noise (diagonal elements of V)</p>
</dd>
<dt><cite>xTest</cite>: 2D numpy array of size nTestxp</dt><dd><p>Test samples from the input (parameter) space</p>
</dd>
<dt><cite>gprOpts</cite>: dict</dt><dd><dl>
<dt>Options for constructing GPR with the following keys:</dt><dd><ul>
<li><dl class="simple">
<dt>‘nIter’: int</dt><dd><p>Number of iterations in the optimization of hyperparameters</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>‘lr’: float </dt><dd><p>Learning rate in the optimization of hyperparameters</p>
</dd>
</dl>
</li>
<li><dl>
<dt>‘standardizeYTrain’: bool (optional, default:False)</dt><dd><p>If true, the training data are standardized by shifting by mean and scaling by sdev:</p>
<p><span class="math notranslate nohighlight">\(yStnd =  (yTrain-mean(yTrain))/sdev(yTrain)\)</span></p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>‘convPlot’: bool </dt><dd><p>If true, optimized values of the hyper-parameters is plotted vs. iteration.</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</dd>
</dl>
</dd>
<dt>Attributes: </dt><dd><p><cite>post_f</cite>: Posterior of f(x) at <cite>xTest</cite>.</p>
<p><cite>post_obs</cite>: Predictive posterior (likelihood) at <cite>xTest</cite>.</p>
</dd>
</dl>
<dl class="py method">
<dt id="gpr_torch.gpr.gprTorch_1d">
<code class="sig-name descname">gprTorch_1d</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#gpr_torch.gpr.gprTorch_1d" title="Permalink to this definition">¶</a></dt>
<dd><p>GPR for 1D uncertain parameter.
Observations <span class="math notranslate nohighlight">\({(x_i,y_i)}_{i=1}^n\)</span> are assumed to be independent but their noise 
variance can be either the same (iid=homoscedastic) or non-identical (heteroscedastic).</p>
</dd></dl>

<dl class="py method">
<dt id="gpr_torch.gpr.gprTorch_1d_singleTask">
<code class="sig-name descname">gprTorch_1d_singleTask</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#gpr_torch.gpr.gprTorch_1d_singleTask" title="Permalink to this definition">¶</a></dt>
<dd><p>GPR for 1D uncertain parameter and single-variate response y.</p>
</dd></dl>

<dl class="py method">
<dt id="gpr_torch.gpr.gprTorch_pd">
<code class="sig-name descname">gprTorch_pd</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#gpr_torch.gpr.gprTorch_pd" title="Permalink to this definition">¶</a></dt>
<dd><p>GPR for p-D (p&gt;1) uncertain parameter.
Observations (X_i,Y_i) are assumed to be independent but their noise variance can 
be either the same (iid=homoscedastic) or different (heteroscedastic).</p>
</dd></dl>

<dl class="py method">
<dt id="gpr_torch.gpr.gprTorch_pd_singleTask">
<code class="sig-name descname">gprTorch_pd_singleTask</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#gpr_torch.gpr.gprTorch_pd_singleTask" title="Permalink to this definition">¶</a></dt>
<dd><p>GPR for p&gt;1 uncertain parameter and single-variate response y</p>
</dd></dl>

<dl class="py method">
<dt id="gpr_torch.gpr.optim_conv_plot">
<code class="sig-name descname">optim_conv_plot</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#gpr_torch.gpr.optim_conv_plot" title="Permalink to this definition">¶</a></dt>
<dd><p>Plot convergence of loss and length-scale during the optimization</p>
</dd></dl>

<dl class="py method">
<dt id="gpr_torch.gpr.train_pred">
<code class="sig-name descname">train_pred</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#gpr_torch.gpr.train_pred" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructor of the GPR (training and predicting at test samples)</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="gpr_torch.gprPlot">
<em class="property">class </em><code class="sig-prename descclassname">gpr_torch.</code><code class="sig-name descname">gprPlot</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pltOpts</span><span class="o">=</span><span class="default_value">{}</span></em><span class="sig-paren">)</span><a class="headerlink" href="#gpr_torch.gprPlot" title="Permalink to this definition">¶</a></dt>
<dd><p>Plotters for GPR</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt><cite>pltOpts</cite>: dict (optional) </dt><dd><dl class="simple">
<dt>Options for planar plots (p=1 or 2) with the following keys:</dt><dd><ul class="simple">
<li><p>‘title’: string, plot title</p></li>
<li><p>‘legFS’: float, legend fontsize</p></li>
<li><p>‘labFS’: [float,float], x,y-axes label fontsize</p></li>
<li><p>‘ticksFS’: [float,float], x,y-ticks fontsize</p></li>
<li><dl class="simple">
<dt>‘save’: bool   </dt><dd><dl class="simple">
<dt>If ‘save’==True, then</dt><dd><ul>
<li><p>‘figName’: string, figure name</p></li>
<li><p>‘figDir’: string, directory to save the figure</p></li>
<li><p>‘figSize’: [float,float], figure size</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</dd>
</dl>
</dd>
<dt>Methods:    </dt><dd><dl class="simple">
<dt><cite>torch1d()</cite>: </dt><dd><p>Plots the GPR constructed for a 1D input.</p>
</dd>
<dt><cite>torch2d_2dcont()</cite>: </dt><dd><p>Planar contour plot of a GPR constructed over a 2D input space.</p>
</dd>
<dt><cite>torch2d_3dSurf()</cite>: </dt><dd><p>3D plot of the GPR surface (mean+CI) constructed for a 2D input.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt id="gpr_torch.gprPlot.torch1d">
<code class="sig-name descname">torch1d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">post_f</span></em>, <em class="sig-param"><span class="n">post_obs</span></em>, <em class="sig-param"><span class="n">xTrain</span></em>, <em class="sig-param"><span class="n">yTrain</span></em>, <em class="sig-param"><span class="n">xTest</span></em>, <em class="sig-param"><span class="n">fExTest</span></em>, <em class="sig-param"><span class="n">shift</span><span class="o">=</span><span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">scale</span><span class="o">=</span><span class="default_value">1.0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#gpr_torch.gprPlot.torch1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Plots the GPR constructed by GPyToch for a 1D input.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt><cite>post_f</cite>: GpyTorch object</dt><dd><p>Posterior density of the model function f(q)</p>
</dd>
<dt><cite>post_obs</cite>: GpyTorch object</dt><dd><p>Posterior density of the response y</p>
</dd>
<dt><cite>xTrain</cite>: 1D numpy array of size nTrain</dt><dd><p>GPR training samples taken from the input space</p>
</dd>
<dt><cite>fExTest</cite>: 1D numpy array of size nTrain</dt><dd><p>Response values at <cite>xTrain</cite></p>
</dd>
<dt><cite>xTest</cite>: 1D numpy array of size nTest</dt><dd><p>Test samples taken from the input space</p>
</dd>
<dt><cite>fExTest</cite>: 1D numpy array of size nTest</dt><dd><p>Exact response values at <cite>xTest</cite></p>
</dd>
<dt><cite>shift</cite>: Float scalar (optional)         </dt><dd><p>Value by which the original training data ‘yTrain’ were shifted for standardization</p>
</dd>
<dt><cite>scale</cite>: Float scalar (optional)         </dt><dd><p>Value by which the original training data ‘yTrain’ were scaled for standardization</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="gpr_torch.gprPlot.torch2d_2dcont">
<code class="sig-name descname">torch2d_2dcont</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">xTrain</span></em>, <em class="sig-param"><span class="n">qTest</span></em>, <em class="sig-param"><span class="n">fTestGrid</span></em><span class="sig-paren">)</span><a class="headerlink" href="#gpr_torch.gprPlot.torch2d_2dcont" title="Permalink to this definition">¶</a></dt>
<dd><p>Planar contour plots of a GPR constructed over a 2D input space.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt><cite>xTrain</cite>: 2D numpy array of shape (nTrain,2)</dt><dd><p>GPR training samples taken from the input space</p>
</dd>
<dt><cite>yTrain</cite>: 1D numpy array of size nTrain</dt><dd><p>Response values at <cite>xTrain</cite></p>
</dd>
<dt><cite>qTest</cite>: List of length 2</dt><dd><p>=[qTest_1,qTest2], where qTest_i: 1D array of size nTest_i of the test 
samples taken from the space of i-th input</p>
</dd>
<dt><cite>fTestGrid</cite>: 2D numpy array of shape (nTest_1,nTest_2)    </dt><dd><p>Response values at a tensor-product grid constructed from <cite>qTest</cite></p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="gpr_torch.gprPlot.torch2d_3dSurf">
<code class="sig-name descname">torch2d_3dSurf</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">xTrain</span></em>, <em class="sig-param"><span class="n">yTrain</span></em>, <em class="sig-param"><span class="n">qTest</span></em>, <em class="sig-param"><span class="n">post_</span></em>, <em class="sig-param"><span class="n">shift</span><span class="o">=</span><span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">scale</span><span class="o">=</span><span class="default_value">1.0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#gpr_torch.gprPlot.torch2d_3dSurf" title="Permalink to this definition">¶</a></dt>
<dd><p>3D plot of the GPR surface (mean+CI) constructed for a 2D input (parameter).</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt><cite>xTrain</cite>: 2D numpy array of shape (nTrain,2)</dt><dd><p>GPR training samples taken from the input space</p>
</dd>
<dt><cite>yTrain</cite>: 1D numpy array of size nTrain</dt><dd><p>Response values at <cite>xTrain</cite></p>
</dd>
<dt><cite>qTest</cite>: List of length 2</dt><dd><p>=[qTest_1,qTest2], where qTest_i: 1D array of size nTest_i of the test 
samples taken from the space of i-th input</p>
</dd>
<dt><cite>post_</cite>: GpyTorch object</dt><dd><p>Posterior density of model function f(q) or observations y</p>
</dd>
<dt><cite>shift</cite>: Float scalar (optional)         </dt><dd><p>Value by which the original training data ‘yTrain’ were shifted for standardization</p>
</dd>
<dt><cite>scale</cite>: Float scalar (optional)         </dt><dd><p>Value by which the original training data ‘yTrain’ were scaled for standardization</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="gpr_torch.gprPost">
<em class="property">class </em><code class="sig-prename descclassname">gpr_torch.</code><code class="sig-name descname">gprPost</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">gpPost</span></em>, <em class="sig-param"><span class="n">nTest</span></em>, <em class="sig-param"><span class="n">shift</span><span class="o">=</span><span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">scale</span><span class="o">=</span><span class="default_value">1.0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#gpr_torch.gprPost" title="Permalink to this definition">¶</a></dt>
<dd><p>Post-processing a constructed GPR</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt><cite>gpPost</cite>: GpyTorch object</dt><dd><p>GPR posterior density created by GPyTorch</p>
</dd>
<dt><cite>nTest</cite>: A list of size p</dt><dd><p>Containing number of test points in each parameter dimension: [nTest1,nTest2,…nTestp]</p>
</dd>
<dt><cite>shift</cite>: Float scalar (optional)         </dt><dd><p>Value by which the original training data ‘yTrain’ were shifted for standardization</p>
</dd>
<dt><cite>scale</cite>: Float scalar (optional)         </dt><dd><p>Value by which the original training data ‘yTrain’ were scaled for standardization</p>
</dd>
</dl>
</dd>
<dt>Methods:</dt><dd><p><cite>torchPost()</cite>: Computing mean, standard-deviation, and CI of the GPR-posterior</p>
</dd>
<dt>Attributes:</dt><dd><dl class="simple">
<dt><cite>mean</cite>: p-D numpy array of size (nTest1,nTest2,…,nTestp)</dt><dd><p>Mean of the GP-posterior</p>
</dd>
<dt><cite>sdev</cite>: pD numpy array of size (nTest1,nTest2,…,nTestp)</dt><dd><p>Standard-deviation of the GP-posterior</p>
</dd>
<dt><cite>ciL</cite>: pD numpy array of size (nTest1,nTest2,…,nTestp)</dt><dd><p>Lower 95% CI of the GP-posterior</p>
</dd>
<dt><cite>ciU</cite>: pD numpy array of size (nTest1,nTest2,…,nTestp)</dt><dd><p>Upper 95% CI of the GP-posterior</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt id="gpr_torch.gprPost.torchPost">
<code class="sig-name descname">torchPost</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#gpr_torch.gprPost.torchPost" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes mean, standard-deviation, and CI of the GPR-posterior created by GPyTorch</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="id17">
<h3>Notebook<a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h3>
<p>Try <a class="reference internal" href="../examples/gpr.html"><span class="doc">GPR notebook</span></a> to see how to use <code class="code docutils literal notranslate"><span class="pre">UQit</span></code> for Gaussian process regression over a parameter space.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="uqFWD.html" class="btn btn-neutral float-right" title="UQ Forward Problem" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="sampling.html" class="btn btn-neutral float-left" title="Sampling" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Saleh Rezaeiravesh

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>